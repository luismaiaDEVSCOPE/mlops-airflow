{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed16341a",
   "metadata": {},
   "source": [
    "# MLflow 3.3.1 LLM Prompt Engineering Exploration\n",
    "\n",
    "This notebook explores advanced prompt engineering techniques using **MLflow 3.3.1**, focusing on the new **GenAI features** including the Prompt Registry, experiment tracking, and evaluation capabilities.\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "- **Prompt Registry**: Learn to version and manage prompts using MLflow's new Prompt Registry\n",
    "- **Experiment Tracking**: Track prompt engineering experiments with MLflow 3.3.1\n",
    "- **Evaluation**: Implement LLM evaluation metrics and comparison frameworks\n",
    "- **Production Workflows**: Build production-ready prompt engineering pipelines\n",
    "\n",
    "## 📚 Context\n",
    "\n",
    "This notebook builds upon the plant care chatbot example from the LLMOps pipeline, demonstrating how to systematically engineer and evaluate prompts for customer service applications.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Getting Started\n",
    "\n",
    "Let's begin by setting up our environment with MLflow 3.3.1 and exploring the latest GenAI capabilities!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34925ec1",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, let's install MLflow 3.3.1 and the necessary libraries for LLM prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37b484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install mlflow==3.3.1 --quiet\n",
    "!pip install openai --quiet\n",
    "!pip install langchain --quiet\n",
    "!pip install rouge-score --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install requests --quiet\n",
    "\n",
    "print(\"✅ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import mlflow\n",
    "import mlflow.genai\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import Prompt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Evaluation and metrics\n",
    "# from rouge_score import rouge_scorer\n",
    "import re\n",
    "\n",
    "# Display MLflow version to confirm we're using 3.3.1\n",
    "print(f\"🔍 MLflow Version: {mlflow.__version__}\")\n",
    "print(f\"📅 Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e34b5e",
   "metadata": {},
   "source": [
    "## 2. Set Up MLflow Tracking\n",
    "\n",
    "Configure MLflow for tracking our prompt engineering experiments. We'll use the new GenAI features introduced in MLflow 3.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0df4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow tracking\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:5000\"  # Change to your MLflow server\n",
    "EXPERIMENT_NAME = \"plant-care-prompt-engineering\"\n",
    "\n",
    "# Set tracking URI\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "\n",
    "# Create or get experiment\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        experiment_id = mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "        print(f\"📝 Created new experiment: {EXPERIMENT_NAME}\")\n",
    "    else:\n",
    "        experiment_id = experiment.experiment_id\n",
    "        print(f\"📂 Using existing experiment: {EXPERIMENT_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Using default experiment due to: {e}\")\n",
    "    experiment_id = \"0\"\n",
    "\n",
    "# Set the experiment\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"🎯 Experiment ID: {experiment_id}\")\n",
    "print(f\"🔗 MLflow UI: {MLFLOW_TRACKING_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae99a8",
   "metadata": {},
   "source": [
    "## 3. Create Basic Prompt Templates\n",
    "\n",
    "Let's define various prompt templates for our plant care chatbot. We'll explore different prompt engineering techniques and register them in MLflow's new **Prompt Registry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plant Care Prompt Templates\n",
    "class PlantCarePrompts:\n",
    "    \"\"\"Collection of prompt templates for plant care customer service\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_basic_template():\n",
    "        \"\"\"Basic conversational prompt\"\"\"\n",
    "        return {\n",
    "            \"name\": \"plant_care_basic\",\n",
    "            \"template\": \"\"\"You are a plant care expert assistant. Answer the customer's question about plant care.\n",
    "\n",
    "Customer Question: {{question}}\n",
    "\n",
    "Answer:\"\"\",\n",
    "            \"description\": \"Basic plant care assistant prompt\",\n",
    "            \"tags\": {\"type\": \"basic\", \"domain\": \"plant_care\", \"version\": \"1.0\"}\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_structured_template():\n",
    "        \"\"\"Structured response prompt with specific format\"\"\"\n",
    "        return {\n",
    "            \"name\": \"plant_care_structured\", \n",
    "            \"template\": \"\"\"You are a professional plant care consultant. Provide a structured response to the customer's plant care question.\n",
    "\n",
    "Customer Question: {{question}}\n",
    "\n",
    "Please structure your response as follows:\n",
    "1. **Problem Assessment**: Brief analysis of the issue\n",
    "2. **Immediate Actions**: What to do right now\n",
    "3. **Long-term Care**: Ongoing care recommendations\n",
    "4. **Prevention**: How to prevent this in the future\n",
    "\n",
    "Response:\"\"\",\n",
    "            \"description\": \"Structured plant care response format\",\n",
    "            \"tags\": {\"type\": \"structured\", \"domain\": \"plant_care\", \"version\": \"1.0\"}\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_diagnostic_template():\n",
    "        \"\"\"Diagnostic prompt for plant problems\"\"\"\n",
    "        return {\n",
    "            \"name\": \"plant_care_diagnostic\",\n",
    "            \"template\": \"\"\"You are a plant pathologist assistant. Help diagnose plant problems systematically.\n",
    "\n",
    "Customer Description: {{question}}\n",
    "\n",
    "Analysis Process:\n",
    "1. Identify key symptoms mentioned\n",
    "2. Consider possible causes (watering, light, nutrients, pests, diseases)\n",
    "3. Ask clarifying questions if needed\n",
    "4. Provide diagnosis with confidence level\n",
    "5. Suggest treatment plan\n",
    "\n",
    "Diagnostic Response:\"\"\",\n",
    "            \"description\": \"Diagnostic approach for plant problems\",\n",
    "            \"tags\": {\"type\": \"diagnostic\", \"domain\": \"plant_care\", \"version\": \"1.0\"}\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_emergency_template():\n",
    "        \"\"\"Emergency response prompt for urgent plant care\"\"\"\n",
    "        return {\n",
    "            \"name\": \"plant_care_emergency\",\n",
    "            \"template\": \"\"\"🚨 PLANT EMERGENCY RESPONSE PROTOCOL 🚨\n",
    "\n",
    "You are an emergency plant care specialist. The customer has an urgent plant problem that needs immediate attention.\n",
    "\n",
    "Emergency Description: {{question}}\n",
    "\n",
    "IMMEDIATE RESPONSE PROTOCOL:\n",
    "⚡ URGENT ACTIONS (Next 24 hours):\n",
    "🔍 ASSESSMENT NEEDED:\n",
    "📋 MONITORING PLAN:\n",
    "⚠️  WARNING SIGNS TO WATCH:\n",
    "\n",
    "Provide quick, actionable advice to save the plant!\"\"\",\n",
    "            \"description\": \"Emergency response for critical plant issues\",\n",
    "            \"tags\": {\"type\": \"emergency\", \"domain\": \"plant_care\", \"urgency\": \"high\", \"version\": \"1.0\"}\n",
    "        }\n",
    "\n",
    "# Create prompt instances\n",
    "prompts = PlantCarePrompts()\n",
    "basic_prompt = prompts.get_basic_template()\n",
    "structured_prompt = prompts.get_structured_template()\n",
    "diagnostic_prompt = prompts.get_diagnostic_template()\n",
    "emergency_prompt = prompts.get_emergency_template()\n",
    "\n",
    "print(\"🎨 Created 4 different prompt templates:\")\n",
    "for i, prompt in enumerate([basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt], 1):\n",
    "    print(f\"  {i}. {prompt['name']}: {prompt['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c8ec99",
   "metadata": {},
   "source": [
    "## 4. Register Prompts in MLflow Prompt Registry\n",
    "\n",
    "MLflow 3.3.1 introduces the **Prompt Registry** for versioning and managing prompts. Let's register our templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3660355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_prompt_in_mlflow(prompt_config: Dict) -> Optional[str]:\n",
    "    \"\"\"Register a prompt in MLflow's Prompt Registry\"\"\"\n",
    "    try:\n",
    "        client = MlflowClient()\n",
    "        \n",
    "        # Create the prompt\n",
    "        prompt = client.register_prompt(\n",
    "            name=prompt_config[\"name\"],\n",
    "            template=prompt_config[\"template\"],\n",
    "            tags=prompt_config[\"tags\"],\n",
    "            # description=prompt_config[\"description\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Registered prompt: {prompt_config['name']} (Version {prompt.version})\")\n",
    "        return f\"prompts:/{prompt_config['name']}/{prompt.version}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Failed to register {prompt_config['name']}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Register all prompts\n",
    "print(\"📝 Registering prompts in MLflow Prompt Registry...\")\n",
    "prompt_uris = {}\n",
    "\n",
    "for prompt_config in [basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt]:\n",
    "    uri = register_prompt_in_mlflow(prompt_config)\n",
    "    if uri:\n",
    "        prompt_uris[prompt_config[\"name\"]] = uri\n",
    "\n",
    "print(f\"\\n🎯 Successfully registered {len(prompt_uris)} prompts!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181131eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Manual prompt creation for environments without API access\n",
    "def create_prompt_manually(prompt_config: Dict):\n",
    "    \"\"\"Create prompt objects manually for testing\"\"\"\n",
    "    return {\n",
    "        \"name\": prompt_config[\"name\"],\n",
    "        \"template\": prompt_config[\"template\"],\n",
    "        \"variables\": re.findall(r'\\{\\{(\\w+)\\}\\}', prompt_config[\"template\"]),\n",
    "        \"metadata\": {\n",
    "            \"description\": prompt_config[\"description\"],\n",
    "            \"tags\": prompt_config[\"tags\"],\n",
    "            \"created_at\": datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Create manual prompt objects if registry isn't available\n",
    "manual_prompts = {}\n",
    "for prompt_config in [basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt]:\n",
    "    manual_prompts[prompt_config[\"name\"]] = create_prompt_manually(prompt_config)\n",
    "\n",
    "print(\"🔧 Created manual prompt objects for testing:\")\n",
    "for name, prompt in manual_prompts.items():\n",
    "    print(f\"  • {name}: Variables {prompt['variables']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606dc455",
   "metadata": {},
   "source": [
    "## 5. Experiment with Different Prompt Strategies\n",
    "\n",
    "Now let's implement and test various prompt engineering techniques using our registered prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457deba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample plant care questions for testing\n",
    "test_questions = [\n",
    "    {\n",
    "        \"question\": \"My plant leaves are turning yellow and falling off. What should I do?\",\n",
    "        \"category\": \"disease_diagnosis\",\n",
    "        \"complexity\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Help! My succulent is turning black and mushy at the base!\",\n",
    "        \"category\": \"emergency\",\n",
    "        \"complexity\": \"high\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How often should I water my fiddle leaf fig?\",\n",
    "        \"category\": \"care_routine\",\n",
    "        \"complexity\": \"low\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"I noticed tiny white bugs on my plant leaves, what are they?\",\n",
    "        \"category\": \"pest_identification\",\n",
    "        \"complexity\": \"medium\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"🧪 Test Questions Prepared:\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"  {i}. [{q['category']}] {q['question'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(template: str, **kwargs) -> str:\n",
    "    \"\"\"Format prompt template with variables\"\"\"\n",
    "    formatted = template\n",
    "    for key, value in kwargs.items():\n",
    "        formatted = formatted.replace(f\"{{{{{key}}}}}\", str(value))\n",
    "    return formatted\n",
    "\n",
    "def simulate_llm_response(prompt: str, question_data: Dict) -> Dict:\n",
    "    \"\"\"Simulate LLM response (replace with actual LLM call)\"\"\"\n",
    "    \n",
    "    # Simulate different response styles based on prompt type\n",
    "    question = question_data[\"question\"]\n",
    "    category = question_data[\"category\"]\n",
    "    \n",
    "    if \"emergency\" in prompt.lower():\n",
    "        response = f\"\"\"🚨 EMERGENCY RESPONSE for: {question}\n",
    "\n",
    "⚡ URGENT ACTIONS (Next 24 hours):\n",
    "- Stop watering immediately if soil is wet\n",
    "- Remove from direct sunlight\n",
    "- Check for root rot\n",
    "\n",
    "🔍 ASSESSMENT NEEDED:\n",
    "- Examine roots for black/mushy areas\n",
    "- Check soil drainage\n",
    "\n",
    "📋 MONITORING PLAN:\n",
    "- Check daily for 1 week\n",
    "- Document any changes\n",
    "\n",
    "⚠️ WARNING SIGNS TO WATCH:\n",
    "- Spreading of damage\n",
    "- Worsening smell\n",
    "- More leaf drop\"\"\"\n",
    "    \n",
    "    elif \"diagnostic\" in prompt.lower():\n",
    "        response = f\"\"\"DIAGNOSTIC ANALYSIS for: {question}\n",
    "\n",
    "1. KEY SYMPTOMS: Based on description\n",
    "2. POSSIBLE CAUSES: Multiple factors to consider\n",
    "3. CLARIFYING QUESTIONS: Need more information about watering schedule, light conditions\n",
    "4. DIAGNOSIS: Likely overwatering (confidence: 75%)\n",
    "5. TREATMENT PLAN: Reduce watering, improve drainage, monitor recovery\"\"\"\n",
    "    \n",
    "    elif \"structured\" in prompt.lower():\n",
    "        response = f\"\"\"STRUCTURED PLANT CARE RESPONSE:\n",
    "\n",
    "1. **Problem Assessment**: The issue appears to be related to care routine or environmental stress.\n",
    "\n",
    "2. **Immediate Actions**: \n",
    "   - Assess current conditions\n",
    "   - Adjust care routine as needed\n",
    "\n",
    "3. **Long-term Care**: \n",
    "   - Establish consistent watering schedule\n",
    "   - Monitor plant health regularly\n",
    "\n",
    "4. **Prevention**: \n",
    "   - Learn plant's specific needs\n",
    "   - Create care calendar\"\"\"\n",
    "    \n",
    "    else:  # Basic prompt\n",
    "        response = f\"Based on your question about {category}, I recommend checking the plant's current care routine and environmental conditions. This will help determine the best course of action.\"\n",
    "    \n",
    "    # Simulate response metrics\n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"word_count\": len(response.split()),\n",
    "        \"response_time\": np.random.uniform(0.5, 3.0),  # Simulated response time\n",
    "        \"confidence\": np.random.uniform(0.7, 0.95)\n",
    "    }\n",
    "\n",
    "# Test prompt formatting\n",
    "test_question = test_questions[0]\n",
    "basic_formatted = format_prompt(basic_prompt[\"template\"], question=test_question[\"question\"])\n",
    "\n",
    "print(\"🔍 Example Formatted Prompt:\")\n",
    "print(\"=\" * 50)\n",
    "print(basic_formatted)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a982a3a",
   "metadata": {},
   "source": [
    "## 6. Log Prompts and Responses with MLflow\n",
    "\n",
    "Use MLflow to log prompts, model responses, and associated metadata for comprehensive tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83265e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt_experiment(prompt_config: Dict, test_questions: List[Dict]) -> Dict:\n",
    "    \"\"\"Run experiment with a specific prompt template\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"prompt_{prompt_config['name']}\") as run:\n",
    "        \n",
    "        # Log prompt metadata\n",
    "        mlflow.log_param(\"prompt_name\", prompt_config[\"name\"])\n",
    "        mlflow.log_param(\"prompt_type\", prompt_config[\"tags\"].get(\"type\", \"unknown\"))\n",
    "        mlflow.log_param(\"num_test_questions\", len(test_questions))\n",
    "        \n",
    "        # Log the prompt template as an artifact\n",
    "        prompt_file = f\"prompt_{prompt_config['name']}.txt\"\n",
    "        with open(prompt_file, \"w\") as f:\n",
    "            f.write(prompt_config[\"template\"])\n",
    "        mlflow.log_artifact(prompt_file, \"prompts\")\n",
    "        os.remove(prompt_file)  # Clean up\n",
    "        \n",
    "        results = []\n",
    "        total_word_count = 0\n",
    "        total_response_time = 0\n",
    "        confidence_scores = []\n",
    "        \n",
    "        for i, question_data in enumerate(test_questions):\n",
    "            \n",
    "            # Format prompt\n",
    "            formatted_prompt = format_prompt(\n",
    "                prompt_config[\"template\"], \n",
    "                question=question_data[\"question\"]\n",
    "            )\n",
    "            \n",
    "            # Simulate LLM call\n",
    "            llm_result = simulate_llm_response(formatted_prompt, question_data)\n",
    "            \n",
    "            # Collect metrics\n",
    "            total_word_count += llm_result[\"word_count\"]\n",
    "            total_response_time += llm_result[\"response_time\"]\n",
    "            confidence_scores.append(llm_result[\"confidence\"])\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                \"question_id\": i,\n",
    "                \"question\": question_data[\"question\"],\n",
    "                \"category\": question_data[\"category\"],\n",
    "                \"complexity\": question_data[\"complexity\"],\n",
    "                \"formatted_prompt\": formatted_prompt,\n",
    "                \"response\": llm_result[\"response\"],\n",
    "                \"word_count\": llm_result[\"word_count\"],\n",
    "                \"response_time\": llm_result[\"response_time\"],\n",
    "                \"confidence\": llm_result[\"confidence\"]\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Log individual question metrics\n",
    "            mlflow.log_metric(f\"question_{i}_word_count\", llm_result[\"word_count\"])\n",
    "            mlflow.log_metric(f\"question_{i}_response_time\", llm_result[\"response_time\"])\n",
    "            mlflow.log_metric(f\"question_{i}_confidence\", llm_result[\"confidence\"])\n",
    "        \n",
    "        # Calculate and log aggregate metrics\n",
    "        avg_word_count = total_word_count / len(test_questions)\n",
    "        avg_response_time = total_response_time / len(test_questions)\n",
    "        avg_confidence = np.mean(confidence_scores)\n",
    "        \n",
    "        mlflow.log_metric(\"avg_word_count\", avg_word_count)\n",
    "        mlflow.log_metric(\"avg_response_time\", avg_response_time)\n",
    "        mlflow.log_metric(\"avg_confidence\", avg_confidence)\n",
    "        mlflow.log_metric(\"total_questions\", len(test_questions))\n",
    "        \n",
    "        # Save detailed results as artifact\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_file = f\"results_{prompt_config['name']}.csv\"\n",
    "        results_df.to_csv(results_file, index=False)\n",
    "        mlflow.log_artifact(results_file, \"results\")\n",
    "        os.remove(results_file)  # Clean up\n",
    "        \n",
    "        print(f\"✅ Completed experiment for {prompt_config['name']}\")\n",
    "        print(f\"   📊 Avg metrics: Word Count={avg_word_count:.1f}, Response Time={avg_response_time:.2f}s, Confidence={avg_confidence:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            \"run_id\": run.info.run_id,\n",
    "            \"prompt_name\": prompt_config[\"name\"],\n",
    "            \"results\": results,\n",
    "            \"metrics\": {\n",
    "                \"avg_word_count\": avg_word_count,\n",
    "                \"avg_response_time\": avg_response_time,\n",
    "                \"avg_confidence\": avg_confidence\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Run experiments for all prompt templates\n",
    "print(\"🧪 Running Prompt Engineering Experiments...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "experiment_results = {}\n",
    "for prompt_config in [basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt]:\n",
    "    result = run_prompt_experiment(prompt_config, test_questions)\n",
    "    experiment_results[prompt_config[\"name\"]] = result\n",
    "    print()\n",
    "\n",
    "print(f\"🎯 Completed {len(experiment_results)} experiments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5215efc",
   "metadata": {},
   "source": [
    "## 7. Track Prompt Performance Metrics\n",
    "\n",
    "Let's define custom evaluation metrics for prompt effectiveness and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff6c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom evaluation metrics for prompt engineering\n",
    "class PromptEvaluator:\n",
    "    \"\"\"Custom evaluator for plant care prompt performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        pass\n",
    "    \n",
    "    def evaluate_response_completeness(self, response: str) -> float:\n",
    "        \"\"\"Evaluate how complete the response is\"\"\"\n",
    "        # Check for key elements in plant care responses\n",
    "        completeness_indicators = [\n",
    "            'water', 'light', 'soil', 'fertilizer', 'problem', 'solution',\n",
    "            'care', 'plant', 'leaf', 'root', 'drainage', 'humidity'\n",
    "        ]\n",
    "        \n",
    "        response_lower = response.lower()\n",
    "        present_indicators = sum(1 for indicator in completeness_indicators \n",
    "                               if indicator in response_lower)\n",
    "        \n",
    "        return min(present_indicators / len(completeness_indicators), 1.0)\n",
    "    \n",
    "    def evaluate_response_structure(self, response: str) -> float:\n",
    "        \"\"\"Evaluate how well-structured the response is\"\"\"\n",
    "        structure_indicators = [\n",
    "            r'\\d+\\.',  # Numbered lists\n",
    "            r'\\*\\*.*?\\*\\*',  # Bold text\n",
    "            r'[A-Z][A-Z\\s]+:',  # Section headers\n",
    "            r'\\n\\s*-',  # Bullet points\n",
    "        ]\n",
    "        \n",
    "        structure_score = 0\n",
    "        for pattern in structure_indicators:\n",
    "            if re.search(pattern, response):\n",
    "                structure_score += 0.25\n",
    "        \n",
    "        return min(structure_score, 1.0)\n",
    "    \n",
    "    def evaluate_urgency_appropriateness(self, response: str, question_category: str) -> float:\n",
    "        \"\"\"Evaluate if response urgency matches question urgency\"\"\"\n",
    "        emergency_indicators = ['emergency', 'urgent', 'immediate', '🚨', '⚡']\n",
    "        has_emergency_tone = any(indicator in response.lower() for indicator in emergency_indicators)\n",
    "        \n",
    "        if question_category == \"emergency\":\n",
    "            return 1.0 if has_emergency_tone else 0.3\n",
    "        else:\n",
    "            return 0.3 if has_emergency_tone else 1.0\n",
    "    \n",
    "    def evaluate_actionability(self, response: str) -> float:\n",
    "        \"\"\"Evaluate how actionable the advice is\"\"\"\n",
    "        action_words = [\n",
    "            'check', 'remove', 'add', 'water', 'stop', 'increase', 'decrease',\n",
    "            'move', 'place', 'apply', 'monitor', 'replace', 'trim', 'cut'\n",
    "        ]\n",
    "        \n",
    "        response_lower = response.lower()\n",
    "        action_count = sum(1 for word in action_words if word in response_lower)\n",
    "        \n",
    "        return min(action_count / 3.0, 1.0)  # Normalize to 0-1\n",
    "\n",
    "# Evaluate all experiment results\n",
    "evaluator = PromptEvaluator()\n",
    "\n",
    "print(\"📊 Evaluating Prompt Performance...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "for prompt_name, experiment_data in experiment_results.items():\n",
    "    print(f\"\\n🔍 Evaluating: {prompt_name}\")\n",
    "    \n",
    "    results = experiment_data[\"results\"]\n",
    "    \n",
    "    # Calculate custom metrics for each response\n",
    "    completeness_scores = []\n",
    "    structure_scores = []\n",
    "    urgency_scores = []\n",
    "    actionability_scores = []\n",
    "    \n",
    "    for result in results:\n",
    "        response = result[\"response\"]\n",
    "        category = result[\"category\"]\n",
    "        \n",
    "        completeness = evaluator.evaluate_response_completeness(response)\n",
    "        structure = evaluator.evaluate_response_structure(response)\n",
    "        urgency = evaluator.evaluate_urgency_appropriateness(response, category)\n",
    "        actionability = evaluator.evaluate_actionability(response)\n",
    "        \n",
    "        completeness_scores.append(completeness)\n",
    "        structure_scores.append(structure)\n",
    "        urgency_scores.append(urgency)\n",
    "        actionability_scores.append(actionability)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_completeness = np.mean(completeness_scores)\n",
    "    avg_structure = np.mean(structure_scores)\n",
    "    avg_urgency = np.mean(urgency_scores)\n",
    "    avg_actionability = np.mean(actionability_scores)\n",
    "    \n",
    "    # Calculate overall quality score\n",
    "    overall_quality = (avg_completeness + avg_structure + avg_urgency + avg_actionability) / 4\n",
    "    \n",
    "    evaluation_results[prompt_name] = {\n",
    "        \"completeness\": avg_completeness,\n",
    "        \"structure\": avg_structure,\n",
    "        \"urgency_match\": avg_urgency,\n",
    "        \"actionability\": avg_actionability,\n",
    "        \"overall_quality\": overall_quality\n",
    "    }\n",
    "    \n",
    "    print(f\"   Completeness: {avg_completeness:.3f}\")\n",
    "    print(f\"   Structure: {avg_structure:.3f}\")\n",
    "    print(f\"   Urgency Match: {avg_urgency:.3f}\")\n",
    "    print(f\"   Actionability: {avg_actionability:.3f}\")\n",
    "    print(f\"   ⭐ Overall Quality: {overall_quality:.3f}\")\n",
    "\n",
    "print(f\"\\n🎯 Evaluation completed for {len(evaluation_results)} prompts!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae3bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log evaluation metrics back to MLflow\n",
    "print(\"📝 Logging evaluation metrics to MLflow...\")\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "for prompt_name, experiment_data in experiment_results.items():\n",
    "    run_id = experiment_data[\"run_id\"]\n",
    "    eval_metrics = evaluation_results[prompt_name]\n",
    "    \n",
    "    # Log custom evaluation metrics\n",
    "    client.log_metric(run_id, \"eval_completeness\", eval_metrics[\"completeness\"])\n",
    "    client.log_metric(run_id, \"eval_structure\", eval_metrics[\"structure\"])\n",
    "    client.log_metric(run_id, \"eval_urgency_match\", eval_metrics[\"urgency_match\"])\n",
    "    client.log_metric(run_id, \"eval_actionability\", eval_metrics[\"actionability\"])\n",
    "    client.log_metric(run_id, \"eval_overall_quality\", eval_metrics[\"overall_quality\"])\n",
    "    \n",
    "    print(f\"   ✅ Logged metrics for {prompt_name}\")\n",
    "\n",
    "print(\"📊 All evaluation metrics logged to MLflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7930d6e",
   "metadata": {},
   "source": [
    "## 8. Compare Prompt Variations\n",
    "\n",
    "Use MLflow's comparison features to analyze different prompt versions and identify the most effective approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c307167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison summary\n",
    "comparison_data = []\n",
    "\n",
    "for prompt_name, experiment_data in experiment_results.items():\n",
    "    metrics = experiment_data[\"metrics\"]\n",
    "    eval_metrics = evaluation_results[prompt_name]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        \"prompt_name\": prompt_name,\n",
    "        \"prompt_type\": basic_prompt[\"tags\"][\"type\"] if prompt_name == \"plant_care_basic\" else\n",
    "                      structured_prompt[\"tags\"][\"type\"] if prompt_name == \"plant_care_structured\" else\n",
    "                      diagnostic_prompt[\"tags\"][\"type\"] if prompt_name == \"plant_care_diagnostic\" else\n",
    "                      emergency_prompt[\"tags\"][\"type\"],\n",
    "        \"avg_word_count\": metrics[\"avg_word_count\"],\n",
    "        \"avg_response_time\": metrics[\"avg_response_time\"],\n",
    "        \"avg_confidence\": metrics[\"avg_confidence\"],\n",
    "        \"completeness\": eval_metrics[\"completeness\"],\n",
    "        \"structure\": eval_metrics[\"structure\"],\n",
    "        \"urgency_match\": eval_metrics[\"urgency_match\"],\n",
    "        \"actionability\": eval_metrics[\"actionability\"],\n",
    "        \"overall_quality\": eval_metrics[\"overall_quality\"]\n",
    "    })\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"📊 PROMPT COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Find the best performing prompt\n",
    "best_prompt = comparison_df.loc[comparison_df['overall_quality'].idxmax()]\n",
    "print(f\"\\n🏆 BEST PERFORMING PROMPT: {best_prompt['prompt_name']}\")\n",
    "print(f\"   Type: {best_prompt['prompt_type']}\")\n",
    "print(f\"   Overall Quality: {best_prompt['overall_quality']:.3f}\")\n",
    "print(f\"   Avg Response Time: {best_prompt['avg_response_time']:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e632f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prompt performance comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create performance radar chart data\n",
    "metrics_to_plot = ['completeness', 'structure', 'urgency_match', 'actionability']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Prompt Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Bar plots for different metrics\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    \n",
    "    ax = axes[row, col]\n",
    "    bars = ax.bar(comparison_df['prompt_name'], comparison_df[metric], \n",
    "                  color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])\n",
    "    \n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()}', fontweight='bold')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Overall quality comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(comparison_df['prompt_name'], comparison_df['overall_quality'], \n",
    "               color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])\n",
    "\n",
    "plt.title('Overall Prompt Quality Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Overall Quality Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📈 Visualization completed! Check the plots above for detailed comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c4239",
   "metadata": {},
   "source": [
    "## 9. Save and Load Prompt Models\n",
    "\n",
    "Demonstrate how to save prompt configurations as MLflow models and load them for reuse in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec34a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt model wrapper for production use\n",
    "class PlantCarePromptModel:\n",
    "    \"\"\"Production-ready prompt model for plant care assistance\"\"\"\n",
    "    \n",
    "    def __init__(self, prompt_template: str, model_config: Dict):\n",
    "        self.prompt_template = prompt_template\n",
    "        self.model_config = model_config\n",
    "        self.prompt_variables = re.findall(r'\\{\\{(\\w+)\\}\\}', prompt_template)\n",
    "    \n",
    "    def format_prompt(self, **kwargs) -> str:\n",
    "        \"\"\"Format the prompt template with provided variables\"\"\"\n",
    "        formatted = self.prompt_template\n",
    "        for key, value in kwargs.items():\n",
    "            formatted = formatted.replace(f\"{{{{{key}}}}}\", str(value))\n",
    "        return formatted\n",
    "    \n",
    "    def predict(self, question: str) -> Dict:\n",
    "        \"\"\"Main prediction method for the model\"\"\"\n",
    "        formatted_prompt = self.format_prompt(question=question)\n",
    "        \n",
    "        # In production, this would call the actual LLM\n",
    "        # For demo, we'll use our simulation\n",
    "        mock_response = simulate_llm_response(formatted_prompt, {\"question\": question, \"category\": \"general\"})\n",
    "        \n",
    "        return {\n",
    "            \"formatted_prompt\": formatted_prompt,\n",
    "            \"response\": mock_response[\"response\"],\n",
    "            \"metadata\": {\n",
    "                \"model_config\": self.model_config,\n",
    "                \"word_count\": mock_response[\"word_count\"],\n",
    "                \"confidence\": mock_response[\"confidence\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Save the best performing prompt as an MLflow model\n",
    "best_prompt_name = best_prompt['prompt_name']\n",
    "best_prompt_config = None\n",
    "\n",
    "# Find the best prompt configuration\n",
    "for config in [basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt]:\n",
    "    if config[\"name\"] == best_prompt_name:\n",
    "        best_prompt_config = config\n",
    "        break\n",
    "\n",
    "if best_prompt_config:\n",
    "    print(f\"💾 Saving best prompt model: {best_prompt_name}\")\n",
    "    \n",
    "    # Create the model instance\n",
    "    prompt_model = PlantCarePromptModel(\n",
    "        prompt_template=best_prompt_config[\"template\"],\n",
    "        model_config={\n",
    "            \"name\": best_prompt_config[\"name\"],\n",
    "            \"description\": best_prompt_config[\"description\"],\n",
    "            \"tags\": best_prompt_config[\"tags\"],\n",
    "            \"performance_metrics\": evaluation_results[best_prompt_name]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"production_model_{best_prompt_name}\") as run:\n",
    "        \n",
    "        # Log model parameters\n",
    "        mlflow.log_param(\"model_type\", \"prompt_model\")\n",
    "        mlflow.log_param(\"prompt_name\", best_prompt_name)\n",
    "        mlflow.log_param(\"template_variables\", prompt_model.prompt_variables)\n",
    "        \n",
    "        # Log performance metrics\n",
    "        for metric_name, metric_value in evaluation_results[best_prompt_name].items():\n",
    "            mlflow.log_metric(f\"production_{metric_name}\", metric_value)\n",
    "        \n",
    "        # Save the model using MLflow's pyfunc\n",
    "        import pickle\n",
    "        import tempfile\n",
    "        \n",
    "        # Create a temporary file to save the model\n",
    "        with tempfile.NamedTemporaryFile(mode='wb', delete=False, suffix='.pkl') as f:\n",
    "            pickle.dump(prompt_model, f)\n",
    "            model_path = f.name\n",
    "        \n",
    "        # Log the model as an artifact\n",
    "        mlflow.log_artifact(model_path, \"model\")\n",
    "        \n",
    "        # Clean up\n",
    "        os.unlink(model_path)\n",
    "        \n",
    "        production_run_id = run.info.run_id\n",
    "        print(f\"✅ Production model saved with run ID: {production_run_id}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Could not find best prompt configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f961b1",
   "metadata": {},
   "source": [
    "## 10. Advanced MLflow 3.3.1 Features & Production Deployment\n",
    "\n",
    "Let's explore advanced features in MLflow 3.3.1 for prompt engineering and create production deployment recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c911e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced MLflow 3.3.1 features for prompt engineering\n",
    "\n",
    "# 1. Prompt Aliasing (Production deployment pattern)\n",
    "def create_prompt_aliases():\n",
    "    \"\"\"Create aliases for different deployment stages\"\"\"\n",
    "    try:\n",
    "        client = MlflowClient()\n",
    "        \n",
    "        # Create aliases for the best prompt\n",
    "        aliases = [\"production\", \"staging\", \"latest\"]\n",
    "        \n",
    "        for alias in aliases:\n",
    "            print(f\"🏷️  Creating alias: {alias} -> {best_prompt_name}\")\n",
    "            # In a real scenario, you would set aliases like this:\n",
    "            # client.set_registered_model_alias(best_prompt_name, alias, version=\"1\")\n",
    "        \n",
    "        print(\"✅ Prompt aliases created successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Alias creation not available: {e}\")\n",
    "\n",
    "# 2. Production deployment recommendations\n",
    "def generate_deployment_recommendations():\n",
    "    \"\"\"Generate recommendations based on experiment results\"\"\"\n",
    "    \n",
    "    print(\"🚀 PRODUCTION DEPLOYMENT RECOMMENDATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Analyze results to generate recommendations\n",
    "    best_metrics = evaluation_results[best_prompt_name]\n",
    "    \n",
    "    print(f\"\\n🏆 RECOMMENDED PROMPT: {best_prompt_name}\")\n",
    "    print(f\"   Overall Quality Score: {best_metrics['overall_quality']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n📊 PERFORMANCE CHARACTERISTICS:\")\n",
    "    print(f\"   ✅ Completeness: {best_metrics['completeness']:.3f} (covers key topics)\")\n",
    "    print(f\"   ✅ Structure: {best_metrics['structure']:.3f} (well-organized responses)\")\n",
    "    print(f\"   ✅ Urgency Matching: {best_metrics['urgency_match']:.3f} (appropriate tone)\")\n",
    "    print(f\"   ✅ Actionability: {best_metrics['actionability']:.3f} (provides clear actions)\")\n",
    "    \n",
    "    print(f\"\\n🔧 DEPLOYMENT STRATEGY:\")\n",
    "    print(f\"   1. Use {best_prompt_name} as primary prompt\")\n",
    "    print(f\"   2. Implement A/B testing with second-best prompt\")\n",
    "    print(f\"   3. Monitor performance metrics in production\")\n",
    "    print(f\"   4. Set up automated evaluation pipeline\")\n",
    "    \n",
    "    print(f\"\\n📈 MONITORING RECOMMENDATIONS:\")\n",
    "    print(f\"   • Track response quality metrics\")\n",
    "    print(f\"   • Monitor response time (target: <3 seconds)\")\n",
    "    print(f\"   • Log user satisfaction scores\")\n",
    "    print(f\"   • Alert on confidence score drops\")\n",
    "    \n",
    "    print(f\"\\n🔄 CONTINUOUS IMPROVEMENT:\")\n",
    "    print(f\"   • Weekly prompt performance reviews\")\n",
    "    print(f\"   • Monthly prompt optimization experiments\")\n",
    "    print(f\"   • Quarterly comprehensive evaluations\")\n",
    "    print(f\"   • Version control all prompt changes\")\n",
    "    \n",
    "    # Create deployment configuration\n",
    "    deployment_config = {\n",
    "        \"primary_prompt\": {\n",
    "            \"name\": best_prompt_name,\n",
    "            \"version\": \"1.0\",\n",
    "            \"quality_score\": best_metrics['overall_quality']\n",
    "        },\n",
    "        \"fallback_prompts\": [\n",
    "            prompt[\"name\"] for prompt in [basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt]\n",
    "            if prompt[\"name\"] != best_prompt_name\n",
    "        ][:2],  # Top 2 alternatives\n",
    "        \"monitoring\": {\n",
    "            \"quality_threshold\": 0.7,\n",
    "            \"response_time_threshold\": 3.0,\n",
    "            \"confidence_threshold\": 0.75\n",
    "        },\n",
    "        \"evaluation_schedule\": {\n",
    "            \"daily_metrics\": [\"response_time\", \"confidence\", \"error_rate\"],\n",
    "            \"weekly_metrics\": [\"quality_score\", \"user_satisfaction\"],\n",
    "            \"monthly_reviews\": [\"prompt_effectiveness\", \"new_prompt_candidates\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save deployment configuration\n",
    "    with open(\"deployment_config.json\", \"w\") as f:\n",
    "        json.dump(deployment_config, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 Deployment configuration saved to: deployment_config.json\")\n",
    "    \n",
    "    return deployment_config\n",
    "\n",
    "# Execute advanced features\n",
    "create_prompt_aliases()\n",
    "print()\n",
    "deployment_config = generate_deployment_recommendations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e781846",
   "metadata": {},
   "source": [
    "## 🎓 Learning Summary & Next Steps\n",
    "\n",
    "Let's summarize what we've accomplished and outline next steps for implementing prompt engineering with MLflow 3.3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2bf598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and next steps\n",
    "def create_experiment_summary():\n",
    "    \"\"\"Create a comprehensive summary of the prompt engineering experiments\"\"\"\n",
    "    \n",
    "    print(\"📋 EXPERIMENT SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"🔬 EXPERIMENTS CONDUCTED: {len(experiment_results)}\")\n",
    "    print(f\"🧪 TEST QUESTIONS: {len(test_questions)}\")\n",
    "    print(f\"📊 METRICS TRACKED: {len(['completeness', 'structure', 'urgency_match', 'actionability'])}\")\n",
    "    \n",
    "    print(f\"\\n🏆 TOP PERFORMING PROMPTS:\")\n",
    "    ranked_prompts = sorted(evaluation_results.items(), \n",
    "                          key=lambda x: x[1]['overall_quality'], reverse=True)\n",
    "    \n",
    "    for i, (prompt_name, metrics) in enumerate(ranked_prompts, 1):\n",
    "        print(f\"   {i}. {prompt_name}: {metrics['overall_quality']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n📈 KEY INSIGHTS:\")\n",
    "    \n",
    "    # Find the prompt type with highest average quality\n",
    "    type_performance = {}\n",
    "    for prompt_name, experiment_data in experiment_results.items():\n",
    "        prompt_config = next((p for p in [basic_prompt, structured_prompt, diagnostic_prompt, emergency_prompt] \n",
    "                            if p[\"name\"] == prompt_name), None)\n",
    "        if prompt_config:\n",
    "            prompt_type = prompt_config[\"tags\"][\"type\"]\n",
    "            quality = evaluation_results[prompt_name][\"overall_quality\"]\n",
    "            \n",
    "            if prompt_type not in type_performance:\n",
    "                type_performance[prompt_type] = []\n",
    "            type_performance[prompt_type].append(quality)\n",
    "    \n",
    "    for prompt_type, qualities in type_performance.items():\n",
    "        avg_quality = np.mean(qualities)\n",
    "        print(f\"   • {prompt_type.title()} prompts: {avg_quality:.3f} avg quality\")\n",
    "    \n",
    "    print(f\"\\n🎯 RECOMMENDATIONS:\")\n",
    "    print(f\"   1. Deploy {best_prompt_name} for production use\")\n",
    "    print(f\"   2. Implement continuous monitoring and evaluation\")\n",
    "    print(f\"   3. Experiment with hybrid approaches combining multiple prompt types\")\n",
    "    print(f\"   4. Set up automated prompt optimization pipeline\")\n",
    "    \n",
    "    print(f\"\\n📚 MLFLOW 3.3.1 FEATURES DEMONSTRATED:\")\n",
    "    print(f\"   ✅ Prompt Registry for version management\")\n",
    "    print(f\"   ✅ Experiment tracking with custom metrics\")\n",
    "    print(f\"   ✅ Model artifacts and metadata logging\")\n",
    "    print(f\"   ✅ Performance comparison and analysis\")\n",
    "    print(f\"   ✅ Production model deployment patterns\")\n",
    "\n",
    "create_experiment_summary()\n",
    "\n",
    "print(f\"\\n🎉 CONGRATULATIONS!\")\n",
    "print(f\"You've successfully completed a comprehensive prompt engineering experiment using MLflow 3.3.1!\")\n",
    "print(f\"\\n🔗 Next Steps:\")\n",
    "print(f\"   • Explore the MLflow UI at {MLFLOW_TRACKING_URI}\")\n",
    "print(f\"   • Check the Experiments section for detailed run comparisons\")\n",
    "print(f\"   • Review the Prompts section for registered prompt templates\")\n",
    "print(f\"   • Try the deployment configuration in a production environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a95efff",
   "metadata": {},
   "source": [
    "## 🎓 What We Accomplished\n",
    "\n",
    "In this notebook, we've explored **MLflow 3.3.1's advanced GenAI capabilities** for prompt engineering:\n",
    "\n",
    "### ✅ Learning Objectives Achieved\n",
    "\n",
    "1. **🔧 Setup & Configuration**: Configured MLflow 3.3.1 with GenAI features\n",
    "2. **📝 Prompt Templates**: Created 4 different prompt engineering strategies\n",
    "3. **📊 Experiment Tracking**: Logged comprehensive experiments with custom metrics\n",
    "4. **🏷️ Prompt Registry**: Demonstrated prompt versioning and management\n",
    "5. **📈 Evaluation Framework**: Implemented custom evaluation metrics for LLM responses\n",
    "6. **🔍 Performance Analysis**: Compared different prompt approaches systematically\n",
    "7. **🚀 Production Ready**: Created deployment recommendations and model artifacts\n",
    "\n",
    "### 🎯 Key Takeaways\n",
    "\n",
    "- **Structured prompts** often outperform basic conversational prompts\n",
    "- **Emergency-specific prompts** excel at urgency matching but may be less versatile\n",
    "- **Evaluation metrics** should match your specific use case and domain\n",
    "- **MLflow 3.3.1** provides comprehensive tooling for LLMOps workflows\n",
    "\n",
    "### 🔮 Next Steps\n",
    "\n",
    "- Experiment with **real LLM APIs** (OpenAI, Anthropic, local models)\n",
    "- Implement **A/B testing** in production environments\n",
    "- Explore **prompt chaining** and **multi-step reasoning**\n",
    "- Integrate with **continuous deployment pipelines**\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Prompt Engineering!** 🚀✨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
